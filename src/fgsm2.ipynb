{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"lenet_mnist_model.pth\" #事前学習済みMNISTモデル(重みパラメータ)\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LeNet Model 定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) #畳み込み層nn.Conv2d(入力のチャネル数, 出力のチャネル数，カーネルの1辺のサイズ)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d() #過学習を防ぐためにいくつかのノードを無効にする\n",
    "        self.fc1 = nn.Linear(320, 50) #全結合層nn.Linear(入力のサイズ(20channel×4height×4width), 出力サイズ) height, weightについては計算して事前に出す\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) #活性化関数Relu, Maxプーリング\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320) #サイズを調整x.view(-1, 指定するサイズ)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# MNISTのTest datasetと dataloaderの定義\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ])), \n",
    "        batch_size=1, shuffle=False)\n",
    "\n",
    "# 使うデバイス（CPUかGPUか）の定義\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# ネットワークの初期化\n",
    "model = Net().to(device)\n",
    "\n",
    "# 訓練済みモデルのロード\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "# モデルを評価モードに設定。本チュートリアルの例では、これはドロップアウト層等を評価モードにするのに必要\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM による攻撃用のコード\n",
    "def fgsm_attack(image,  data_grad):\n",
    "    # データの勾配の各要素のsign値を取得します\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # 入力画像の各ピクセルを調整して、ノイズが追加された画像を作成します\n",
    "    perturbed_image = image + 0.1*sign_data_grad\n",
    "    # [0,1]の範囲になるようデータをクリップします\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # ノイズが追加された画像を返します\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, data_grad):\n",
    "    data_grad_sum = 0\n",
    "    for i in data_grad[0][0]:\n",
    "        for j in i:\n",
    "            data_grad_sum += j\n",
    "    \n",
    "    data_grad_avg = data_grad_sum / (28 * 28)\n",
    "    perturbed_image = image + (data_grad > data_grad_avg * 10).float() - (data_grad < -data_grad_avg * 10).float()\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, data_grad):\n",
    "    data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "    threshold_p = heapq.nlargest(25, data_grad_chain)[24]\n",
    "    threshold_n = heapq.nsmallest(25, data_grad_chain)[24] \n",
    "    # threshold_p = torch.Tensor([heapq.nlargest(15, data_grad_chain)[14]]).to(\"cuda\")\n",
    "    # threshold_n = torch.Tensor([heapq.nsmallest(15, data_grad_chain)[14]]).to(\"cuda\")\n",
    "    # for i in range(28):\n",
    "    #     for j in range(28):\n",
    "    #         if data_grad[0][0][i][j] >= threshold_p:\n",
    "    #             image[0][0][i][j] = torch.Tensor([1]).to(\"cuda\")\n",
    "    #         if data_grad[0][0][i][j] <= threshold_n:\n",
    "    #             image[0][0][i][j] = torch.Tensor([-1]).to(\"cuda\")\n",
    "    # perturbed_image = image + (data_grad[0][0] >= threshold_p).float() - (data_grad[0][0] <= threshold_n).float()\n",
    "    perturbed_image = image + (data_grad >= threshold_p).float() - (data_grad <= threshold_n).float()\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, data_grad):\n",
    "    data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "    threshold = heapq.nlargest(20, list(map(abs, data_grad_chain)))[19]\n",
    "    perturbed_image = image + (data_grad >= threshold).float() - (data_grad <= -threshold).float()\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, data_grad):\n",
    "    data_grad_max = max(data_grad)\n",
    "    data_grad_min = min(data_grad)\n",
    "    perturbed_image = image + (data_grad == data_grad_max).float() - (data_grad == data_grad_min).float()\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "\n",
    "    # 精度カウンター\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # テスト用データセット内の全てのサンプルをループします\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # データとラベルをデバイス（CPUもしくはGPU）に送信します\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # テンソルの requires_grad 属性を設定します。攻撃者にとっては重要な設定です。\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # データをモデルに順伝播させます\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # 最大の確率のインデックスを取得します。\n",
    "\n",
    "        # 最初から予測が間違っている場合、攻撃する必要がないため次のイテレーションに進みます。\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # 損失を計算します\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # 既存の勾配を全てゼロにします\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 逆伝播させてモデルの勾配を計算します\n",
    "        loss.backward()\n",
    "\n",
    "        # データの勾配を取得します\n",
    "        data_grad = data.grad.data\n",
    "        \n",
    "        # FGSMによる攻撃の関数を呼び出します\n",
    "        perturbed_data = fgsm_attack(data, data_grad)\n",
    "\n",
    "        # ノイズ付き画像を再度分類します\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # 攻撃の成功を確認します\n",
    "        final_pred = output.max(1, keepdim=True)[1] # log-probabilityが最大のインデックスを取得します\n",
    "\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "        else:\n",
    "            # あとで可視化するために敵対的サンプルのうちいくつかを保存\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "\n",
    "    # epsilonごとの最終的な精度を算出\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Test Accuracy = {} / {} = {}\".format(correct, len(test_loader), final_acc))\n",
    "\n",
    "    # 精度と敵対的サンプルを返却\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 4412 / 10000 = 0.4412\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# 各epsilonごとにテストを実行\n",
    "acc, ex = test(model, device, test_loader)\n",
    "accuracies.append(acc)\n",
    "examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAACECAYAAACd4lHRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASoUlEQVR4nO3deayUxZ7G8acEBBFhUAQVHMAtKMuAS6KIyL1BNkUuoiCgImoEFTU6Orig3ogwGYwYQtxQR9zCMgiiMjKGK0YdY+JFREG4CIjLDAoiizB6Qaz5o49l1evpprvp7tNd/f0kJ/mV1W+/ZdepPkW9tRhrrQAAAGJyUF0XAAAAoNDo4AAAgOjQwQEAANGhgwMAAKJDBwcAAESHDg4AAIgOHRwAABCdiungGGPGGWP+aoz5uzFmZgnv+0djzIfGmJ3GmA3GmGtLde9YGWNONMb8ZIx5oUT3u8YYs84Ys8sYs9gYc0wp7hsj2mFcaIuVyRhzsjHmTWPMjprPc3CJ7ltR7bBiOjiS/lfSA5L+PZ+LjTGt8rimgaQFkp6Q1EzSMElTjTH/lE8Z4Dwi6YNcL8qzDntJmixpkKTDJX0uaVau7wOHdhgX2mKFMcbUl7RQ0mtKfY7XSnrBGHNSDu9RFe2wYjo41tr51tqXJW3N8y2WGmP+Yoy5zBjTOMtrDpfUVNLzNuUDSaslnZJnGaqeMeZSSdsl/SWPy9cZYxYaY/5U09iycYGk/7DWrrLW7pE0UVJPY8zxedy/6tEO40FbrFgdJB0j6WFr7T5r7ZuS/lvS5Tm8R1W0w4rp4BTA6ZKekTRK0v8YY2YYY87KdIG19lul/oUx2hhTr+b1bSW9W/TSRsgY01TS/ZJuzfMtjpX0uqTxkr42xkw1xnTO5ta1xJ3yLAMODO2wDNAWo2OU2+dYFe2wajo41tr/s9a+YK09T1IXSRslzTTGrDHGDM1w6SxJ90r6u6R3JN1trf2q6AWO00RJT1trv87nYmvtdmvt49basyT1lPSTpP+smRPyxzSXLZY01BjTxRhziFJ1aSVl+68WFBDtsGzQFivX3yRtlnS7MaaBMaaPpHOVw+dYLe0wyg6OMWZVzSS2XcaYc2p5ySZJH0taIam1pDZp3qeDpNmSrpB0sKSOkv7FGHN+cUoeL2NMV0m9JT2c5et3eT//WMtLvlCq/lZKOkFSy9rex1q7RNJ9kl5SqhFvlPSDpLy+2JE92mF5oi1WNmvtXkl/knS+pG8k/bOkuUrzOVZzO6xf1wUoBmttx9r+uzGmm1KVM1zSBqWG6K6x1u5M81adJK211v5XTfpvxphFkvpLWlTYUkevl6R2kr40xkhSE0n1jDGnWGtPTb7YWtsk+d9M6sIeStXhEEl/VaoOB1trf0p3Y2vtI0pNplTNRLwJSn0Zo4hoh2Wrl2iLFc1a+7FSozaSJGPMe5KeTfPaqm2HFdPBqZk5Xl9SPaUaYyNJP1trf87y+jclnSTpeUk9rbVrs7hsuaQTa4Zcl0o6TqmJclPy+F+odjOU6v3/6jalvmSvy+E91kv6WamG3CWb4fWa35MTJK1Sat7ADEnTrLXbcrgvatAOo0BbrHDGmC6S1ir1FOZ6SUdLmpnD9dXRDq21FfEj6c9KPa/1f/6cw/VnSTooj/sOVepfGL8Opf5bPu/DT631+UKO1/TI4z7/oNTw626lhnP/VVK9uv7/r9Qf2mF8P7TFyvuR9KCkbZJ2KTXZ+4Qcr6+KdmhqCg0AABCNKCcZAwCA6kYHBwAARIcODgAAiA4dHAAAEB06OAAAIDo57YNjjGHJVZmw1pr9v+r3qMPyQR1G4Ttr7ZG5XkQdlhXqsPLVWoeM4ABA/r6o6wLggFGHla/WOqSDAwAAokMHBwAARIcODgAAiA4dHAAAEB06OAAAIDp0cAAAQHTo4AAAgOjQwQEAANGhgwMAAKJDBwcAAESHDg4AAIhOTodtVoOTTjrJxWvWrAnybr75ZhdPnz69ZGUCAAC5YQQHAABEhw4OAACIDh0cAAAQHebgJHTr1s3Fv/zyS5D39ddfl7o4UTv11FOD9Pz5813crl27otzTWuvivn37BnmrV6928VdffVWU+yM7AwcOdPHChQuDvBtvvNHFjz/+eJC3b9++4hYsEi1btnTx3Llzg7z33nvPxXfeeWfe9zDG5H3tr5o1axake/bs6eLFixcHeXv37j3g+8XI/85LKkQdlTNGcAAAQHTo4AAAgOiYTMNXv3uxMdm/OAI7duwI0v5wafJzK/VQn7U2rxuWcx1u2rTJxUcffXSQ53/eB/JZZ/p9nzNnjosvvfTSvO+RQ1miq8NsJeuhRYsWQfqjjz5ycevWrdO+T/J3oQ6G3JdZa0/P9aJS12Hz5s2D9Nq1a12cfAy0YMECFw8dOjTtexbrs/bLs2zZsiDvyCOPdPFpp50W5K1bty7fW1ZEHeYi27/rBx98cJCu4Md8tdYhIzgAACA6dHAAAEB06OAAAIDoVP0y8U6dOgXplStXuvjFF19Me13sy+uKpX79337lli5dmvV1/ud9IPOf/NdeeeWVQd6tt97q4kMPPTTI2717d9b3wP4l62zw4MFB+thjj017rd8uR4wYUdiCRcSf1+TPL5Okww8/3MWPPvpokOcvwx82bFiRSpfehAkTXNy+ffsgb8yYMS7+7LPPgrzYv5Mzfe+NHDkyyPviiy9cnKktNW3aNEhv3br1QIpYdhjBAQAA0aGDAwAAosMy8QpVqUuMzzvvPBe//vrrQd6UKVNcfNdddwV5pd6N01+OKklbtmwp+D0qtQ6zlWlIvWHDhkHeu+++G6TPOOOMtO/bv39/Fyd/h+pA2S4x7tOnj4szfU5HHXVUkC7G73omHTt2DNKrVq1y8UsvvRTk+Y+Vf/jhh0IVoWzrMJM2bdq4ePny5UHeEUcc4eJst8aQpHHjxgXp77///kCKWEosEwcAANWBDg4AAIgOHRwAABCdql8mnglLhQ9cchn+rFmzXOwvGZekJk2alKRMKI1Mc6M6d+4cpJPb7vv27NkTpJPbyyPFPyFckoYMGeLievXqFeQehZoL58+7WbJkSZDXqlUrF19xxRVBXgHn3VS82267zcX+sv/9Oeig38Y1hg8fHuT169cvSE+aNMnF06dPD/KS7bIcMYIDAACiQwcHAABEp+qWibdr1y5Ib9iwIUj7p+x26NChFEXKS6UsMZ49e3aQHjRokIsPOeSQUhZFUjiUm9y185dffnFxKZbOVkodFsPkyZOD9B133JH2tYsWLQrSF1xwgYvLYPfaslli/Pzzzwfpyy67zL9foW9XMH67k6SZM2e6+KqrripFEcqmDjNp27ZtkPZ3K963b1+Q98knn7j422+/DfL87QOSn33S5s2bXdytW7cg75tvvtlPiUuKZeIAAKA60MEBAADRoYMDAACiU3XLxM8999wg7S+Zk6R33nmnlMWJ0sUXX+ziAQMGBHnr1q0rdXECd999t4uTz5/feustF2/fvr1EJYqLP6cv07yPc845J0gn2+FPP/3k4nvuuSfIGzhw4IEUMVrJ+ZT+7/crr7wS5PlLyPfu3VuQ+yXr259jlzx65frrr0/7PiWad1Nx/Dk3Uli/yb9b/t+5Ro0aBXn+0vBkvRx//PFB2p+LuHDhwiDPPzKlXI90YAQHAABEhw4OAACITtU9onr22Wcz5vsnWiM/l1xyiYsbN24c5D366KMlLUtyW4CRI0e6OLm08oEHHnBxvsP21S7TY6nu3bu7+Oyzz874Pv6u4R999NEBl6vanX/++UH6jTfecHHycexjjz2W1z38XW8lqVevXi4+88wz0143b968vO5XbYYOHRqk/Ud7Dz/8cNrr/Me9kvTMM8+42P+ulqTjjjsuSPuPjpcuXRrksZMxAABAHaCDAwAAokMHBwAARCeaOTiZlqf6z3+T275v3LgxSO9vbgB+r1mzZkE60/P2fJ/v5+vaa68N0i1atHDx6tWrg7zkM2YU1hlnnOHi5LLw5JL95HwO7N+0adOC9B/+8AcXH3PMMUFez549XZz8vrzwwguzul+yzu68884gnekYIP+InORSZfzG/wxffvnltK9LzrHK9Frf6adnf0LF+++/H6R37dqV9bV1hREcAAAQHTo4AAAgOtE8osq0PLV3794u9k+TlqTFixcH6eSSOuxfw4YNg3Tr1q1dPGvWrFIXJ5DcmdO3cuXKEpYE/nB48vFGcqnyE088UYoiRWXZsmVBukuXLi7u2rVrkNevXz8X33777UHeli1bXLy/bTV8ydPMV6xYkfa1J554YtbvW838v2vJZeKDBw92cXIrhQ4dOri4c+fOaa9r3rx5kJdsh347Te5W7Nf3p59+Wlvx6xwjOAAAIDp0cAAAQHTo4AAAgOiYTEv5fvdiY7J/cYXwn0dK0oIFC+qoJLmx1qafdJRBMerQPzVYCk+2bdCgQZDnL10t1gm0LVu2dPGmTZvSvu6mm24K0o888khRypNOOdVhMfTo0SNI+6e1168fTv/7/PPPg3T79u2LVq4CW2atzX6tbY1KqcNcJLf5X7dunYuTc0T69u3rYn/OTx2piDpMzh/N9P3pz53J9Dd+yZIlQfqGG24I0q+99pqLk/OmnnzySRePHTs27T1KpNY6ZAQHAABEhw4OAACIDh0cAAAQnWj2wclXpcy5KWc//vhjkF6/fr2LhwwZEuT5R2VMnTo1r/t16tQpSCef/bdr187FmZ4/J/diQWEdccQRQdo/niH52T/11FMlKROK59577w3SftsbP358kFcG824qTnLOjb+/27x584K8TPvC+e2wUaNGQV5yH7j58+e7+I477gjy/HlUyf3G/L8BdYkRHAAAEB06OAAAIDpRPqJKDqVNnjzZxcmtx0ePHl2SMlWT++67z8XJoVL/1Nt8j3H47rvvgnTyMZR/YngmM2fOzOv+yM7FF18cpP2h8eRw+4wZM0pSJhTOJZdcEqRHjRoVpHfu3OnirVu3lqRM1cRf4p1sayNGjHBx8viFJk2auHh/J4JPnDjRxSeffHKQ5586n3w8mfxdqCuM4AAAgOjQwQEAANGhgwMAAKJTsXNwkvMu/Lkebdu2TXvdtm3bilYmpKxZs8bFQ4cODfK6du3q4hNOOCGv908uiUzy51mNHDky7euSy9tx4Nq0aePi4cOHB3mXX365i5PbzqPy9O/fP0gnv5P9bf4//PDDkpSpWiWPXEim8+V/R86ZMyfI8+fg+EfwSGH7LtaRPNlgBAcAAESHDg4AAIhOxT6iyrRT43XXXRekx4wZ4+JXX321aGXC/vmnCidPGC6UDRs2ZPW65I7IK1euLEZxqkr37t1d7C8Ll8LdiydNmhTk3XPPPcUtGAou+Yhq9+7dQfqhhx4qZXHg8R8XZvpbmYu5c+cGaf8R1bBhw4K8cePGufj+++8vyP3zwQgOAACIDh0cAAAQHTo4AAAgOhU7ByepR48eLl66dGkdlgR1zX/mnOn5M3NuCi95grjPP2Jj2rRppSgOCmzs2LEubtWqVZC3efPmIM3S8LpTqHk3Pn8OnSRNmTLFxYMGDQry/ON6Zs+eHeStXbu24GVLhxEcAAAQHTo4AAAgOtE8oho8eLCL69WrF+QtX77cxW+//XbJyoS64S+RTO6uiuLq06dP2rwvv/zSxTt27ChFcVBg/iOqZNtatGhR2usOO+ywIN28eXMX+78XqBz+Nh/J08QffPBBF0+ePDnI83c0L/Zu8ozgAACA6NDBAQAA0aGDAwAAolOxc3AaN24cpAcMGJD2tf7p0/v27StamVAeGjVqlDaPE8QLq0GDBkE60wnx/me/d+/eopUJdSP53Tpy5EgX33LLLUHeqlWrXDxq1KjiFgxF99xzzwVp/3ikiy66KMjzj274+OOPi1ouRnAAAEB06OAAAIDoVOwjquQQ97Zt21z8yiuvBHnsmlpdRo8e7eLt27cHeRMnTixxaeKW3N30gw8+cHHHjh2DvPXr15ekTKgb11xzTZC++uqrXfz0008HebTDuGzZsiVI9+7d28UbN24M8saPH+9i/zFmMTCCAwAAokMHBwAARIcODgAAiE40c3C6d+9eRyVBufHngUydOjXI46T5wkouDZ4wYYKLk1v5c7p05Rs3bpyL/eW+0u+PwXnsscdc7M+RlKQ9e/YUoXQoF/7xG0uWLAnyLrzwQhefcsopQd6nn35a0HIwggMAAKJDBwcAAETH5HLasjGGo5nLhLXW5HMddVg+qMMoLLPWnp7rRdRhWaEOi6hp06ZBesWKFS6++eabg7zkFi85qLUOGcEBAADRoYMDAACiQwcHAABEp2KXiQMAgPK2c+fOIN2+ffuS3ZsRHAAAEB06OAAAIDp0cAAAQHTo4AAAgOjQwQEAANGhgwMAAKKT6zLx7yR9UYyCICdtD+Ba6rA8UIdxyLceqcPyQR1WvlrrMKezqAAAACoBj6gAAEB06OAAAIDo0MEBAADRoYMDAACiQwcHAABEhw4OAACIDh0cAAAQHTo4AAAgOnRwAABAdP4fJbJFtclbXYgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "\n",
    "for j in range(len(examples[0])):\n",
    "    cnt += 1\n",
    "    plt.subplot(1,len(examples[0]),cnt)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    orig,adv,ex = examples[0][j]\n",
    "    plt.title(\"{} -> {}\".format(orig, adv))\n",
    "    plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
